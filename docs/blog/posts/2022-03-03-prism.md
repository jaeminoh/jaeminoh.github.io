---
date:
  created: 2022-03-03

categories:
  - Statistics
  - Review
---

# Review: Forecasting unemployment using internet search data via PRISM
Author: Dingdong Yi, Sahoyang Ning, Chia-Jung Chang, and S. C. Kou.

Journal: Journal of the american statistical association (2021).

### 요약
1. PRISM 모델은 unemployment forecasting 문제를 state-of-the-art 수준으로 풀었다.

<!-- more -->

### 더 공부할 것
1. BATS - TBATS의 관계를 공부해서 어떻게 trigonometric representation이 prediction performance를 높이 끌어올렸는지 알아보자.
2. STL: seasonality-trend decomposition by LOESS를 공부해서 time series의 seasonality를 따로 빼는 방법을 알아보자. Temperature-mortality relationship에서는 natural cubic spline으로 seasonality를 control하는데, 이 방법을 쓰면 어떻게 될까? (인구증가에 의한 mortality 증가분은 또 따로 생각해야...)
3. PRISM은 state-space model에 기반했는데, 이는 seasonality term과 trend term이 state vector에 의존하는 형태를 가지고 있고, state vector는 이전 state vector의 affine function으로 결정된다.
4. Diebold - Mariano test는 nonparametric model 사이의 prediction performance를 비교할 수 있는 검정이다.

### 소개
실업자의 수가 중요한 경제학 변수라는 데 이의를 제기할 사람은 아마 없을 것이다. 미국에서는 이번 주의 실업자 수를 모두 세서 그 다음 주 목요일에 발표한다. 그 일주일의 공백을 메우는 방법 중 하나를 소개한다.

직장에서 해고당한 사람은 인터넷에 실업과 관련된 무엇을 검색할 가능성이 높다고 생각한다.
실업급여를 받을 수 있는지, 현재 실업자는 증가하는 추세인지, 해고당한 사람이 받을 수 있는 지원에는 무엇이 있는지 등을 집에서 손쉽게 알아볼 수 있는 방법은 인터넷 검색 외에 잘 떠오르지 않는다.
따라서 "unemployment"와 연관된 키워드가 구글에서 얼마나 검색되었는지를 안다면,
역으로 이번 주의 실업자 수를 어느 정도 "nowcasting" 할 수 있을지도 모른다.
주목해야 할 부분은, 실업이 발생했기 때문에 실업과 관련된 검색이 증가한다는 점이다.
따라서 검색 traffic은 설명변수가 아닌 exogenous variable로써 time series prediction에 기여하게 된다.


### 모델 설명
"unemployment"와 관련된 상위 연관 검색어 25개의 구글 검색 횟수, 그리고 실업자 수가 주어져 있다.
현재 시점을 $t$라고 하자. 구글 검색 횟수 데이터는 $\{ x_{1:t} \}$, 실업자 수 데이터는 $\{y_{1:t-1} \}$로 나타낼 수 있다.

현재 실업자 수의 예측은 $\{ y_{t-M, t-1} \}$ 그리고 $x_t$를 이용해 이루어진다. 그렇다고 이 값들을 설명변수로 여기고 penalized linear regression을 한다고 끝은 아니다. 실제 실업자 수를 그래프로 나타낸, 논문의 Figure 1 아래 그림을 보면 매년 반복되는 spike가 보일 것이다. 이런 경향성을 seasonality라고 하고 이것을 제외한 경향성을 trend라고 한다. PRISM에서는 seasonality, trend를 따로 추정한 후 exogenous variable (search traffic)과 함께 $L^1$ penalized linear regression을 해서 time series를 예측한다.

결국, 모델은 두 단계에 걸쳐 설명된다. 첫번째 단계로 seasonality와 trend의 decomposition (via STL)을 진행하고, 두번째 단계로 추정된 seasonality, trend를 검색 traffic과 함께 penalized regression 한다.

첫번째 단계는 논문에서 자세히 기술하지 않았다. Seasonality-trend decomposition based on LOESS (STL) 방법을 사용했다고만 나와있다. 이 방법을 처음 소개한 논문은 피인용수가 상당히 높은데, 한번 읽어볼 필요가 있겠다.

두번째 단계는 첫번째 단계에서 추정된 값들을 설명변수로 여겨서 penalized linear regression을 하는 단계이다. 논문의 식 (1)에 해당되는데, 시계열의 state-space formulation을 통해서 해당 식을 유도할 수 있다. 
$y_t = \gamma_t + z_t$로 분해할 수 있다. $\gamma_t$는 seasonality를 나타내는 항이다. 따라서 $z_t$는 seasonality가 조정된 term으로 볼 수 있다. 

이 두 항들은 linear state-space model에 의해 시간에 따라 변화하는데, $z_t = w^t h_t + \epsilon_t$, $h_t = F h_{t-1} + \eta_t$로 결정된다. $\gamma_t$도 coefficient만 다른, 똑같이 생긴 형태로 결정된다. error term ($\epsilon_t, \eta_t$)는 iid하게 $N(0, H)$를 따른다는 가정이다. 여기서 $h_t$는 state vector라고 불린다. State vector가 이전 state vector에 대해 linear한 함수로 결정되고, 다시 현재 state vector가 trend를 linear한 함수로 결정한다는 모델이다. 
지금까지 정보만 사용한다면, exogenous variable을 포함하지 않은 PRISM을 유도할 수 있다. Google search traffic을 반영한 PRISM을 위해서는, 아주 조금 더 필요하다.

**실업을 검색하면 직장에서 해고되는 것이 아니라, 직장에서 해고되어야 실업을 검색하는 것이다.** 따라서 $x_t \lvert y_t \sim N(\mu_x + y_t \beta, Q)$로 모델링할 수 있다. 
기본 검색량 $\mu_x$에 $t$ 시점의 실업자 수 $y_t$가 검색에 미치는 영향 $\beta$, 그리고 $Q$로 나타내어지는 variability. 물론 이 분포가 Gaussian인지는 따져봐야 할 문제지만, 꽤 합리적으로 보인다. 지금까지의 모델링을 사용해 predictive distribution $p(y_t \lvert \gamma, z, x)$를 얻을 수 있다. (Supple 참조)

State-space model은 predictive distribution의 형태를 유도하기 위해서만 사용되고, 실제 fitting 과정에서 $F, w$ 같은 값들은 추정되지 않는다.

Fitting은 weighted MSE + $L^1$ penalty를 최소화시키는 방향으로 진행된다. 
현재 시점에서 멀어질수록 exponential하게 감소되는 weight를 square error 앞에 곱하고, 
mean을 취해서 weighted MSE를 얻는다. 
예측하고 싶은 지점으로부터 너무 먼 시점의 정보는 그닥 쓸모있지 않을 가능성이 높아서, rolling window size $N$만큼만 과거 정보를 사용한다.
Penalty는 $\gamma, z, x$의 coefficient에 $L^1$ norm을 취한 후 모두 더하고 $\lambda$를 곱한 형태이다.
논문의 식 (5)에는 $x$의 coefficient를 분리한 형태로 적혀있지만, hyperparameter tuning 과정에서 분리를 하지 않아도 비슷한 결과가 나오는 것을 확인했다고 한다.

### Predictive intervals
PRISM은 semi-parametric 방법이다. 따라서 정확한 standard error를 구하기가 불가능하다. 대신 rolling window size $L$에 대해 root mean square 값을 approximated standard error로 사용할 수 있다. 

$\hat{se}^2_t = {1 \over L} \sum_{\tau = t-L}^{t-1} ( \hat{y}_{\tau} - y_{\tau} )^2$

Residual의 normality와, local stationarity를 가정하면 아마 ergodic theorem에 의해 standard error로 수렴할 것이다. Normal QQ plot (supple A10)을 보면 실제로 residual이 normality assumption을 꽤 잘 만족하고 있는 것을 볼 수 있다.

### Accuracy metric
Naive model (전 주의 실업자 수를 이번 주 실업자 수로 예측하는 방법)을 baseline으로 두고, root mean square error 혹은 mean absolute error를 사용하여 여러 모델 간 prediction 성능을 비교한다.

실제로 결과를 보면 Naive model에 비해 엄청나게 정확한 prediction을 생산하지는 않는다. Naive model 대비 에러가 1/4 정도 나오는게 거의 최선. 미래 (또는 현재)를 예측하는게 얼마나 어려운 일인지 이 값들을 보고 알았다.

Diebold - Mariano test를 통해서 여러 모델을 비교하면, 5% 유의수준에서 PRISM의 성능이 모든 다른 모델에 비해 더 좋다고 말할 수 있었다.

### et cetra
논문의 Out of sample test는 coefficient를 더 이상 업데이트 하지 않고 진행했다는 뜻이다.
기간별로 잘라서 해당 기간의 실업자 수를 prediction하는 테스트했을 때, 
특정 기간에서는 PRISM보다 정확한 예측을 하는 모델이 존재했지만 
그 모델은 다른 기간에서는 PRISM보다 안 좋은 예측을 했다.
결과적으로 naive model보다 항상 좋은 결과를 낸 것은 PRISM이 유일했다.

저자는 이런 robustness를 penalization이 sparse한 coefficient를 생산해서,
적당하지 않은 feature를 drop했기 때문이라고 설명했다.

### Reference
1. Dingdong Yi, Shaoyang Ning, Chia-Jung Chang & S. C. Kou (2021) Forecasting Unemployment Using Internet Search Data via PRISM, Journal of the American Statistical Association, 116:536, 1662-1673, DOI: 10.1080/01621459.2021.1883436