---
date:
  created: 2025-04-05

categories:
  - Math

draft: true
---

# Universal approximation theorem

학회, 혹은 인터넷에서 강연을 듣다 보면 neural network가 훌륭한 성능을 보이는 이론적인 뒷받침으로 universal approximation theorem을 언급하는 분들을 종종 볼 수 있었다. 하지만 잘 알려진 Cybenko의 theorem[@cybenko1989approximation]은 denseness에 관한 결과다. Polynomial도 항을 무한히 늘리면 어떤 연속함수든지 전부 approximation 할 수 있다. 따라서 theorem 이름이 멋있어서 생긴 myth이거나, 아니면 진짜 practical 결과를 뒷받침할 수 있는 결과가 있다는 건데, 이걸 알아보고자 포스트를 작성한다.

<!-- more -->

### Theorem 2 (Cybenko, 1989)
Let $\sigma$ be any continuous sigmoidal function. Then finite sums of the form

$$
    G(x) = \sum_{j=1}^N \alpha_j \sigma(y_j^T x + \theta_j)
$$

are dense in $C([0,1]^d)$.

다시 말하면, 임의의 continuous function $g$가 있을 때, 모든 $\epsilon > 0$에 대해 어떤 $\{\alpha_j, y_j, \theta_j \}_{j=1}^N$가 있어서

$$
    \sup_{x \in [0,1]^d} |G(x) - g(x)| < \epsilon
$$

을 만족한다는 뜻이다. 

!!! Note
    여기서 $N$은 고정된 정수가 아니다. 따라서 $G$가 얼마나 효율적으로 $g$를 근사하는지에 대한 정보는 알 수 없다.


그럼 polynomial의 경우는 어떨까?

### Stone-Weierstrass Theorem
Let $X$ be a compact Hausdorff space and $A$ is a subalgebra of $C(X)$ which contains a nonzero constant function. Then $A$ is dense in $C(X)$ if and only if it separates points.[^2]

따라서, $X$를 $[0, 1]^d$라고 하고, $A$를 다변수 다항함수의 집합이라고 하면 모든 가정들이 만족되어, 다변수 다항함수의 집합은 $C([0,1]^d)$에서 dense하다.

Cybenko의 theorem과 Stone-Weierstrass theorem을 비교해보면, 두 집합 모두 $C([0, 1]^d)$ 안에서 dense 하다는 성질을 공통적으로 만족하고 있다.
그렇다면, neural network가 잘 동작하는 이유가 universal approximation theorem 때문이면, 같은 성질을 만족하는 다변수 다항함수도 똑같이 잘 동작해야 하는게 아닐까?
하지만 머신러닝 커뮤니티에서 다항함수는 인기 있는 대상이 아닌데, 내가 생각하는 이유는 다음과 같다.

1. 최근 사용되는 인공신경망은 $G(x)$ 보다 훨씬 복잡하다.
2. 다항함수는 컴퓨터로 계산하기 까다로운 녀석이다.[^3]


[^2]: https://en.wikipedia.org/wiki/Stone–Weierstrass_theorem#Stone–Weierstrass_theorem,_real_version
[^3]: Monomial basis로 matrix를 만들어 least square를 풀게 되면, matrix의 condition number가 너무 커서 근사 퀄리티가 굉장히 좋지 않다.